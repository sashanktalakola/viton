{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c_Z_lDXHark"
   },
   "source": [
    "# Importing and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tc3d8-DJFfeF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zYC3h2uZHwdm"
   },
   "outputs": [],
   "source": [
    "environment = \"local\"\n",
    "\n",
    "if environment == \"local\":\n",
    "  data_root = \"data\"\n",
    "  BATCH_SIZE = 8\n",
    "elif environment == \"kaggle\":\n",
    "  data_root = \"/kaggle/input/marquis-viton-hd\"\n",
    "  BATCH_SIZE = 64\n",
    "\n",
    "train_folder = os.path.join(data_root, \"train\")\n",
    "valid_folder = os.path.join(data_root, \"test\")\n",
    "\n",
    "train_file_names = os.listdir(os.path.join(train_folder, \"image\"))\n",
    "random.shuffle(train_file_names)\n",
    "\n",
    "valid_file_names = os.listdir(os.path.join(valid_folder, \"image\"))\n",
    "random.shuffle(valid_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msashanktalakola2\u001b[0m (\u001b[33msashanktalakola\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sashank/Desktop/viton/wandb/run-20240604_225603-7k1mzjwo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sashanktalakola/viton/runs/7k1mzjwo' target=\"_blank\">v0-baseline-0.2</a></strong> to <a href='https://wandb.ai/sashanktalakola/viton' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sashanktalakola/viton' target=\"_blank\">https://wandb.ai/sashanktalakola/viton</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sashanktalakola/viton/runs/7k1mzjwo' target=\"_blank\">https://wandb.ai/sashanktalakola/viton/runs/7k1mzjwo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sashanktalakola/viton/runs/7k1mzjwo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7e7e5ad70910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "LR = 3e-5\n",
    "EPOCHS = 20\n",
    "experiment_name = \"v0-baseline-0.2\"\n",
    "\n",
    "if environment == \"kaggle\":\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    \n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_api = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "    \n",
    "    wandb.login(key=wandb_api)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"viton\",\n",
    "    name=experiment_name,\n",
    "    tags=[\"torch\", environment, \"P100\", \"1GPU\"],\n",
    "    notes=\"This run uses the correct implementation of dataloader (Here the target is not normalized)\",\n",
    "  \n",
    "    config={\n",
    "    \"learning_rate\": LR,\n",
    "    \"architecture\": \"UNet\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RrSWoLjJ2fv"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fjsDPfCMJ57o"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (384, 512)\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1]),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "label_transforms = A.Compose([\n",
    "  A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1]),\n",
    "  ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_transforms = A.Compose([\n",
    "    A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1]),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SQXyhV-vKfJq"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "  def __init__(self, image_list, image_folder, transforms, label_transforms):\n",
    "    self.image_list = image_list\n",
    "    self.image_folder = image_folder\n",
    "    self.transforms = transforms\n",
    "    self.label_transforms = label_transforms\n",
    "\n",
    "  def __len__(self): return len(self.image_list)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    image_name = self.image_list[i]\n",
    "\n",
    "    agnostic_path = os.path.join(self.image_folder, \"agnostic\", image_name)\n",
    "    cloth_path = os.path.join(self.image_folder, \"cloth\", image_name)\n",
    "    output_img_path = os.path.join(self.image_folder, \"image\", image_name)\n",
    "\n",
    "    agnostic_image = cv2.cvtColor(cv2.imread(agnostic_path), cv2.COLOR_BGR2RGB)\n",
    "    cloth_image = cv2.cvtColor(cv2.imread(cloth_path), cv2.COLOR_BGR2RGB)\n",
    "    output_img_image = cv2.cvtColor(cv2.imread(output_img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    agnostic_image = self.transforms(image=agnostic_image)[\"image\"]\n",
    "    cloth_image = self.transforms(image=cloth_image)[\"image\"]\n",
    "    output_img_image = self.label_transforms(image=output_img_image)[\"image\"]\n",
    "\n",
    "    return agnostic_image, cloth_image, output_img_image.float()\n",
    "\n",
    "train_dataset = TrainDataset(train_file_names, train_folder, train_transforms, label_transforms)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(Dataset):\n",
    "  def __init__(self, image_list, image_folder, transforms, label_transforms):\n",
    "    self.image_list = image_list\n",
    "    self.image_folder = image_folder\n",
    "    self.transforms = transforms\n",
    "    self.label_transforms = label_transforms\n",
    "\n",
    "  def __len__(self): return len(self.image_list)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    image_name = self.image_list[i]\n",
    "\n",
    "    agnostic_path = os.path.join(self.image_folder, \"agnostic\", image_name)\n",
    "    cloth_path = os.path.join(self.image_folder, \"cloth\", image_name)\n",
    "    output_img_path = os.path.join(self.image_folder, \"image\", image_name)\n",
    "\n",
    "    agnostic_image = cv2.cvtColor(cv2.imread(agnostic_path), cv2.COLOR_BGR2RGB)\n",
    "    cloth_image = cv2.cvtColor(cv2.imread(cloth_path), cv2.COLOR_BGR2RGB)\n",
    "    output_img_image = cv2.cvtColor(cv2.imread(output_img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    agnostic_image = self.transforms(image=agnostic_image)[\"image\"]\n",
    "    cloth_image = self.transforms(image=cloth_image)[\"image\"]\n",
    "    output_img_image = self.label_transforms(image=output_img_image)[\"image\"]\n",
    "\n",
    "    return agnostic_image, cloth_image, output_img_image.float()\n",
    "\n",
    "valid_dataset = TrainDataset(valid_file_names, valid_folder, train_transforms, label_transforms)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbakXBSpOUEM"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nTbo4p7eTgDL"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    super(DoubleConv, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.padding = padding\n",
    "\n",
    "    self.double_conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = self.double_conv(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uRXLMaXqOVIV"
   },
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    super(DownSample, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.padding = padding\n",
    "\n",
    "    self.double_conv = DoubleConv(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = self.double_conv(X)\n",
    "    X_pooled = self.pool(X)\n",
    "\n",
    "    return X, X_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JV2G2uIhUPOO"
   },
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    super(UpSample, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.padding = padding\n",
    "\n",
    "    self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    self.double_conv = DoubleConv(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "  def forward(self, X, X_skip):\n",
    "    X = self.up(X)\n",
    "\n",
    "    X_cat = torch.cat((X, X_skip), dim=1)\n",
    "    X_cat = self.double_conv(X_cat)\n",
    "\n",
    "    return X_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4MOFjZgmYhum"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "  def __init__(self, filter_sizes):\n",
    "    super(UNet, self).__init__()\n",
    "\n",
    "    self.filter_sizes = filter_sizes\n",
    "    self.down_sample_blocks = []\n",
    "\n",
    "    current_in_channels = 6\n",
    "    for filter_size in filter_sizes:\n",
    "      self.down_sample_blocks.append(DownSample(current_in_channels, filter_size, kernel_size=3, padding=1).to(device))\n",
    "      current_in_channels = filter_size\n",
    "\n",
    "    self.bottleneck = DoubleConv(filter_sizes[-1], filter_sizes[-1]*2, kernel_size=3, padding=1)\n",
    "\n",
    "    self.up_sample_blocks = []\n",
    "    for filter_size in filter_sizes[::-1]:\n",
    "      self.up_sample_blocks.append(UpSample(filter_size*2, filter_size, kernel_size=3, padding=1).to(device))\n",
    "\n",
    "    self.out_conv = nn.Conv2d(filter_sizes[0], 3, kernel_size=1)\n",
    "\n",
    "\n",
    "  def forward(self, X_agnostic, X_cloth):\n",
    "    X = torch.cat((X_agnostic, X_cloth), dim=1)\n",
    "\n",
    "    X1_skip, X = self.down_sample_blocks[0](X)\n",
    "    X2_skip, X = self.down_sample_blocks[1](X)\n",
    "    X3_skip, X = self.down_sample_blocks[2](X)\n",
    "    X4_skip, X = self.down_sample_blocks[3](X)\n",
    "\n",
    "    X = self.bottleneck(X)\n",
    "\n",
    "    X = self.up_sample_blocks[0](X, X4_skip)\n",
    "    X = self.up_sample_blocks[1](X, X3_skip)\n",
    "    X = self.up_sample_blocks[2](X, X2_skip)\n",
    "    X = self.up_sample_blocks[3](X, X1_skip)\n",
    "\n",
    "    X = self.out_conv(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "35wZSND0a-8d"
   },
   "outputs": [],
   "source": [
    "model = UNet([16, 32, 64, 128]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1RmNJ0PolOI"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, loss_fn, optimizer, epoch):\n",
    "  model.train()\n",
    "\n",
    "  batch_losses = []\n",
    "\n",
    "  pbar = tqdm(dataloader, unit=\"batch\", leave=True, desc=f\"Training : Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "  for batch_idx, (X1, X2, target) in enumerate(pbar):\n",
    "    X1 = X1.to(device)\n",
    "    X2 = X2.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    prediction = model(X1, X2)\n",
    "    loss = loss_fn(prediction, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "    pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "  return batch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, dataloader, loss_fn, epoch):\n",
    "  model.eval()\n",
    "\n",
    "  batch_losses = []\n",
    "\n",
    "  pbar = tqdm(dataloader, unit=\"batch\", leave=True, desc=f\"Validation : Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "  for batch_idx, (X1, X2, target) in enumerate(pbar):\n",
    "    X1 = X1.to(device)\n",
    "    X2 = X2.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      prediction = model(X1, X2)\n",
    "      loss = loss_fn(prediction, target)\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "    pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "  return batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"saved-models/{experiment_name}\", exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_losses = train(model, train_dataloader, loss_fn, optimizer, epoch)\n",
    "  train_epoch_loss = sum(train_losses) / len(train_losses)\n",
    "  print(f\"Epoch [{epoch+1}/{EPOCHS}]\\tLoss: {train_epoch_loss}\")\n",
    "\n",
    "  valid_losses = valid(model, valid_dataloader, loss_fn, epoch)\n",
    "  valid_epoch_loss = sum(valid_losses) / len(valid_losses)\n",
    "  print(f\"Epoch [{epoch+1}/{EPOCHS}]\\tLoss: {valid_epoch_loss}\\n\")\n",
    "\n",
    "  wandb.log({\"train_epoch_loss\": train_epoch_loss, \"valid_epoch_loss\": valid_epoch_loss})\n",
    "  torch.save(model.state_dict(), f\"saved-models/epoch - {epoch}\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4808983,
     "sourceId": 8576879,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30716,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
