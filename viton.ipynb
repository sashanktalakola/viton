{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c_Z_lDXHark"
   },
   "source": [
    "# Importing and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tc3d8-DJFfeF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zYC3h2uZHwdm"
   },
   "outputs": [],
   "source": [
    "data_root = \"data\"\n",
    "\n",
    "train_folder = os.path.join(data_root, \"train\")\n",
    "test_folder = os.path.join(data_root, \"test\")\n",
    "\n",
    "train_file_names = os.listdir(os.path.join(train_folder, \"image\"))\n",
    "random.shuffle(train_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RrSWoLjJ2fv"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fjsDPfCMJ57o"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (384, 512)\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1]),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SQXyhV-vKfJq"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "  def __init__(self, image_list, image_folder, transforms):\n",
    "    self.image_list = image_list\n",
    "    self.image_folder = image_folder\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __len__(self): return len(self.image_list)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    image_name = self.image_list[i]\n",
    "\n",
    "    agnostic_path = os.path.join(self.image_folder, \"agnostic\", image_name)\n",
    "    cloth_path = os.path.join(self.image_folder, \"cloth\", image_name)\n",
    "    output_img_path = os.path.join(self.image_folder, \"image\", image_name)\n",
    "\n",
    "    agnostic_image = cv2.cvtColor(cv2.imread(agnostic_path), cv2.COLOR_BGR2RGB)\n",
    "    cloth_image = cv2.cvtColor(cv2.imread(cloth_path), cv2.COLOR_BGR2RGB)\n",
    "    output_img_image = cv2.cvtColor(cv2.imread(output_img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    agnostic_image = self.transforms(image=agnostic_image)[\"image\"]\n",
    "    cloth_image = self.transforms(image=cloth_image)[\"image\"]\n",
    "    output_img_image = self.transforms(image=output_img_image)[\"image\"]\n",
    "\n",
    "    return agnostic_image, cloth_image, output_img_image\n",
    "\n",
    "train_dataset = TrainDataset(train_file_names, train_folder, train_transforms)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbakXBSpOUEM"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nTbo4p7eTgDL"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    super(DoubleConv, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.padding = padding\n",
    "\n",
    "    self.double_conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = self.double_conv(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uRXLMaXqOVIV"
   },
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    super(DownSample, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.padding = padding\n",
    "\n",
    "    self.double_conv = DoubleConv(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = self.double_conv(X)\n",
    "    X_pooled = self.pool(X)\n",
    "\n",
    "    return X, X_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JV2G2uIhUPOO"
   },
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    super(UpSample, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.padding = padding\n",
    "\n",
    "    self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    self.double_conv = DoubleConv(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "  def forward(self, X, X_skip):\n",
    "    X = self.up(X)\n",
    "\n",
    "    X_cat = torch.cat((X, X_skip), dim=1)\n",
    "    X_cat = self.double_conv(X_cat)\n",
    "\n",
    "    return X_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4MOFjZgmYhum"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "  def __init__(self, filter_sizes):\n",
    "    super(UNet, self).__init__()\n",
    "\n",
    "    self.filter_sizes = filter_sizes\n",
    "    self.down_sample_blocks = []\n",
    "\n",
    "    current_in_channels = 6\n",
    "    for filter_size in filter_sizes:\n",
    "      self.down_sample_blocks.append(DownSample(current_in_channels, filter_size, kernel_size=3, padding=1).to(device))\n",
    "      current_in_channels = filter_size\n",
    "\n",
    "    self.bottleneck = DoubleConv(filter_sizes[-1], filter_sizes[-1]*2, kernel_size=3, padding=1)\n",
    "\n",
    "    self.up_sample_blocks = []\n",
    "    for filter_size in filter_sizes[::-1]:\n",
    "      self.up_sample_blocks.append(UpSample(filter_size*2, filter_size, kernel_size=3, padding=1).to(device))\n",
    "\n",
    "    self.out_conv = nn.Conv2d(filter_sizes[0], 3, kernel_size=1)\n",
    "\n",
    "\n",
    "  def forward(self, X_agnostic, X_cloth):\n",
    "    X = torch.cat((X_agnostic, X_cloth), dim=1)\n",
    "\n",
    "    X1_skip, X = self.down_sample_blocks[0](X)\n",
    "    X2_skip, X = self.down_sample_blocks[1](X)\n",
    "    X3_skip, X = self.down_sample_blocks[2](X)\n",
    "    X4_skip, X = self.down_sample_blocks[3](X)\n",
    "\n",
    "    X = self.bottleneck(X)\n",
    "\n",
    "    X = self.up_sample_blocks[0](X, X4_skip)\n",
    "    X = self.up_sample_blocks[1](X, X3_skip)\n",
    "    X = self.up_sample_blocks[2](X, X2_skip)\n",
    "    X = self.up_sample_blocks[3](X, X1_skip)\n",
    "\n",
    "    X = self.out_conv(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "35wZSND0a-8d"
   },
   "outputs": [],
   "source": [
    "model = UNet([16, 32, 64, 128]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1RmNJ0PolOI"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, loss_fn, optimizer, epoch):\n",
    "  model.train()\n",
    "\n",
    "  batch_losses = []\n",
    "\n",
    "  pbar = tqdm(dataloader, unit=\"batch\", leave=False, desc=f\"Training : Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "  for batch_idx, (X1, X2, target) in enumerate(pbar):\n",
    "    X1 = X1.to(device)\n",
    "    X2 = X2.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    prediction = model(X1, X2)\n",
    "    loss = loss_fn(prediction, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "    pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "  return batch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, dataloader, loss_fn, epoch):\n",
    "  model.eval()\n",
    "\n",
    "  batch_losses = []\n",
    "\n",
    "  pbar = tqdm(dataloader, unit=\"batch\", leave=False, desc=f\"Validation : Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "  for batch_idx, (X1, X2, target) in enumerate(pbar):\n",
    "    X1 = X1.to(device)\n",
    "    X2 = X2.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      prediction = model(X1, X2)\n",
    "      loss = loss_fn(prediction, target)\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "    pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "  return batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  train_losses = train(model, train_dataloader, loss_fn, optimizer, epoch)\n",
    "  print(f\"Epoch [{epoch+1}/{EPOCHS}]\\tLoss: {sum(train_losses)/len(train_losses)}\")\n",
    "\n",
    "  valid_losses = valid(model, train_dataloader, loss_fn, epoch)\n",
    "  print(f\"Epoch [{epoch+1}/{EPOCHS}]\\tLoss: {sum(valid_losses)/len(valid_losses)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
