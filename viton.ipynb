{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ir4PPt7Zz1Lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e911702c-6f71-4cec-b84a-f07280ad1455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "!mkdir -p ~/.kaggle/\n",
        "!cp ./drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d marquis03/marquis-viton-hd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxVE0r2I1qvc",
        "outputId": "aa222c6e-442f-404f-cb4c-1e5f471dce25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.14 / client 1.6.12)\n",
            "Dataset URL: https://www.kaggle.com/datasets/marquis03/marquis-viton-hd\n",
            "License(s): Apache 2.0\n",
            "Downloading marquis-viton-hd.zip to /content\n",
            "100% 4.18G/4.19G [00:59<00:00, 126MB/s] \n",
            "100% 4.19G/4.19G [00:59<00:00, 75.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!unzip -q -d data marquis-viton-hd.zip\n",
        "\n",
        "!rm marquis-viton-hd.zip\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "wREG6lvTEeFn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and Utils"
      ],
      "metadata": {
        "id": "4c_Z_lDXHark"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "data_root = \"/content/data\"\n",
        "\n",
        "train_folder = os.path.join(data_root, \"train\")\n",
        "test_folder = os.path.join(data_root, \"test\")"
      ],
      "metadata": {
        "id": "tc3d8-DJFfeF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_names = os.listdir(os.path.join(train_folder, \"image\"))\n",
        "random.shuffle(train_file_names)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zYC3h2uZHwdm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "_RrSWoLjJ2fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (384, 512)\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "    A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1]),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "fjsDPfCMJ57o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "  def __init__(self, image_list, image_folder, transforms):\n",
        "    self.image_list = image_list\n",
        "    self.image_folder = image_folder\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self): return len(self.image_list)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    image_name = self.image_list[i]\n",
        "\n",
        "    agnostic_path = os.path.join(self.image_folder, \"agnostic\", image_name)\n",
        "    cloth_path = os.path.join(self.image_folder, \"cloth\", image_name)\n",
        "    output_img_path = os.path.join(self.image_folder, \"image\", image_name)\n",
        "\n",
        "    agnostic_image = cv2.cvtColor(cv2.imread(agnostic_path), cv2.COLOR_BGR2RGB)\n",
        "    cloth_image = cv2.cvtColor(cv2.imread(cloth_path), cv2.COLOR_BGR2RGB)\n",
        "    output_img_image = cv2.cvtColor(cv2.imread(output_img_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    agnostic_image = self.transforms(image=agnostic_image)[\"image\"]\n",
        "    cloth_image = self.transforms(image=cloth_image)[\"image\"]\n",
        "    output_img_image = self.transforms(image=output_img_image)[\"image\"]\n",
        "\n",
        "    return agnostic_image, cloth_image, output_img_image\n",
        "\n",
        "train_dataset = TrainDataset(train_file_names, train_folder, train_transforms)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "SQXyhV-vKfJq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b, c = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "hPIzQMlAOJWD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "RbakXBSpOUEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "    super(DoubleConv, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.padding = padding\n",
        "\n",
        "    self.double_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.double_conv(X)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "nTbo4p7eTgDL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "    super(DownSample, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.padding = padding\n",
        "\n",
        "    self.double_conv = DoubleConv(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.double_conv(X)\n",
        "    X_pooled = self.pool(X)\n",
        "\n",
        "    return X, X_pooled"
      ],
      "metadata": {
        "id": "uRXLMaXqOVIV"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "    super(UpSample, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.padding = padding\n",
        "\n",
        "    self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "    self.double_conv = DoubleConv(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "  def forward(self, X, X_skip):\n",
        "    X = self.up(X)\n",
        "\n",
        "    X_cat = torch.cat((X, X_skip), dim=1)\n",
        "    X_cat = self.double_conv(X_cat)\n",
        "\n",
        "    return X_cat"
      ],
      "metadata": {
        "id": "JV2G2uIhUPOO"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, filter_sizes):\n",
        "    super(UNet, self).__init__()\n",
        "\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.down_sample_blocks = []\n",
        "\n",
        "    current_in_channels = 6\n",
        "    for filter_size in filter_sizes:\n",
        "      self.down_sample_blocks.append(DownSample(current_in_channels, filter_size, kernel_size=3, padding=1))\n",
        "      current_in_channels = filter_size\n",
        "\n",
        "    self.bottleneck = DoubleConv(filter_sizes[-1], filter_sizes[-1]*2, kernel_size=3, padding=1)\n",
        "\n",
        "    self.up_sample_blocks = []\n",
        "    for filter_size in filter_sizes[::-1]:\n",
        "      self.up_sample_blocks.append(UpSample(filter_size*2, filter_size, kernel_size=3, padding=1))\n",
        "\n",
        "    self.out_conv = nn.Conv2d(filter_sizes[0], 3, kernel_size=1)\n",
        "\n",
        "\n",
        "  def forward(self, X_agnostic, X_cloth):\n",
        "    X = torch.cat((X_agnostic, X_cloth), dim=1)\n",
        "\n",
        "    X1_skip, X = self.down_sample_blocks[0](X)\n",
        "    X2_skip, X = self.down_sample_blocks[1](X)\n",
        "    X3_skip, X = self.down_sample_blocks[2](X)\n",
        "    X4_skip, X = self.down_sample_blocks[3](X)\n",
        "\n",
        "    X = self.bottleneck(X)\n",
        "\n",
        "    X = self.up_sample_blocks[0](X, X4_skip)\n",
        "    X = self.up_sample_blocks[1](X, X3_skip)\n",
        "    X = self.up_sample_blocks[2](X, X2_skip)\n",
        "    X = self.up_sample_blocks[3](X, X1_skip)\n",
        "\n",
        "    X = self.out_conv(X)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "4MOFjZgmYhum"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet([16, 32, 64, 128]).to(device)"
      ],
      "metadata": {
        "id": "35wZSND0a-8d"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Y1RmNJ0PolOI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eziGa5ZTonJV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}